{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        \"\"\"Calculates the loss as a function of the prediction and the actual.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (float) \n",
    "          The value of the loss for this batch of observations.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def derivative(self, predicted, actual):\n",
    "        \"\"\"The derivative of the loss with respect to the prediction.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives of the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "class SquaredErrorLoss(Loss):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        return np.sum(\n",
    "            (predicted - actual) ** 2\n",
    "        )\n",
    "    \n",
    "    def derivative(self, predicted, actual):\n",
    "        return (\n",
    "            2 * (predicted - actual)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(object):\n",
    "        \n",
    "    def __call__(self, a):\n",
    "        \"\"\"Applies activation function to the values in a layer.\n",
    "        \n",
    "        Args:\n",
    "          a (np.ndarray, float): the values from the previous layer (after \n",
    "            multiplying by the weights.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The values h = g(a).\n",
    "        \"\"\"\n",
    "        return a\n",
    "    \n",
    "    def derivative(self, h):\n",
    "        \"\"\"The derivatives as a function of the outputs at the nodes.\n",
    "        \n",
    "        Args:\n",
    "          h (np.ndarray, float): the outputs h = g(a) at the nodes.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives dh/da.\n",
    "        \"\"\"\n",
    "        return np.ones(h.shape)\n",
    "    \n",
    "class ReLU(ActivationFunction):\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        return np.where(a > 0, a, 0)\n",
    "    \n",
    "    def derivative(self, a):\n",
    "        return np.where(a > 0, 1, 0)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        return 1/(1 + np.exp(-a))\n",
    "    \n",
    "    def derivative(self, a):\n",
    "        return  self.__call__(a) * (1 - self.__call__(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"A data structure for a layer in a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      num_nodes (int): number of nodes in the layer\n",
    "      activation_function (ActivationFunction)\n",
    "      values_pre_activation (np.ndarray, float): most recent values\n",
    "        in layer, before applying activation function\n",
    "      values_post_activation (np.ndarray, float): most recent values\n",
    "        in layer, after applying activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, activation_function=ActivationFunction()):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def get_layer_values(self, values_pre_activation):\n",
    "        \"\"\"Applies activation function to values from previous layer.\n",
    "        \n",
    "        Stores the values (both before and after applying activation \n",
    "        function)\n",
    "        \n",
    "        Args:\n",
    "          values_pre_activation (np.ndarray, float): \n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer before applying the activation function\n",
    "        \n",
    "        Returns: (np.ndarray, float)\n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer after applying the activation function\n",
    "        \"\"\"\n",
    "        self.values_pre_activation = values_pre_activation\n",
    "        self.values_post_activation = self.activation_function(\n",
    "            values_pre_activation\n",
    "        )\n",
    "        return self.values_post_activation\n",
    "    \n",
    "    def get_layer_derivatives(self, values_pre_activation):\n",
    "        return self.activation_function.derivative(\n",
    "            values_pre_activation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNeuralNetwork(object):\n",
    "    \"\"\"A data structure for a fully-connected neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      layers (Layer): A list of Layer objects.\n",
    "      loss (Loss): The loss function to use in training.\n",
    "      learning_rate (float): The learning rate to use in backpropagation.\n",
    "      weights (list, np.ndarray): A list of weight matrices,\n",
    "        length should be len(self.layers) - 1\n",
    "      biases (list, float): A list of bias terms,\n",
    "        length should be equal to len(self.layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, loss, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weight matrices and biases to zeros\n",
    "        self.weights = []\n",
    "        self.updatedWeights = []\n",
    "        self.biases = []\n",
    "        self.updatedBiases = []\n",
    "        mu, sigma = 0, 1\n",
    "        for i in range(1, len(self.layers)):\n",
    "            w = np.matrix(np.random.normal(mu, sigma, (self.layers[i - 1].num_nodes, self.layers[i].num_nodes)))\n",
    "            self.weights.append(w)\n",
    "            self.updatedWeights.append(w)\n",
    "            self.biases.append(np.zeros(self.layers[i].num_nodes))\n",
    "            self.updatedBiases.append(np.zeros(self.layers[i].num_nodes))\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"Predicts the output(s) for a given set of input(s).\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray, float): A (batch size) x self.layers[0].num_nodes array\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          An array of the predicted output labels, length is the batch size\n",
    "        \"\"\"\n",
    "        self.storedValuesZ = [np.mean(inputs, axis = 0)]\n",
    "        self.storedValuesA = [np.mean(inputs, axis = 0)]\n",
    "        a = inputs\n",
    "        self.inputs = inputs\n",
    "        ## Iterate layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ## g(hw + b),  h = previous layer values\n",
    "            if i != len(self.layers) - 1:\n",
    "                z = np.matrix(np.add(a * self.weights[i], np.matrix(self.biases[i])))\n",
    "                self.storedValuesZ.append(np.mean(z, axis = 0))\n",
    "                a = np.matrix(self.layers[i + 1].get_layer_values(z))\n",
    "                self.storedValuesA.append(np.mean(a, axis = 0))\n",
    "        return a\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        h = inputs\n",
    "        ## Iterate layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ## g(hw + b),  h = previous layer values\n",
    "            if i != len(self.layers) - 1:\n",
    "                a = np.matrix(np.add(h * self.updatedWeights[i], np.matrix(self.updatedBiases[i])))\n",
    "                h = self.layers[i+1].get_layer_values(a)\n",
    "        return h\n",
    "    \n",
    "    def backProp(self, predicted, actual):\n",
    "        # Update First weights\n",
    "        dlda = np.mean(self.loss.derivative(predicted, actual), axis = 0).T\n",
    "\n",
    "        dadz = self.layers[-1].get_layer_derivatives(self.storedValuesZ[-1]).T\n",
    "        delta = np.multiply(dlda, dadz)\n",
    "        self.updatedWeights[-1]= self.weights[-1] - np.multiply(self.learning_rate * delta, self.storedValuesA[-2]).T\n",
    "        self.updatedBiases[-1] = self.biases[-1] - np.multiply(self.learning_rate, delta).T\n",
    "        # Update rest of the weights\n",
    "        for l in range(2, len(self.layers)):\n",
    "            z = self.storedValuesZ[-l]\n",
    "            dadz = self.layers[-l].get_layer_derivatives(z)\n",
    "            delta = np.multiply(self.weights[-l + 1] * delta, dadz.T)\n",
    "            self.updatedBiases[-l] = self.biases[-l] - np.multiply(self.learning_rate, delta).T\n",
    "            self.updatedWeights[-l] = self.weights[-l] - np.multiply(self.learning_rate, np.dot(delta, self.storedValuesA[-l - 1])).T\n",
    "        self.weights = self.updatedWeights\n",
    "        self.biases = self.updatedBiases\n",
    "        \n",
    "    def train(self, inputs, labels):\n",
    "        \"\"\"Trains neural network based on a batch of training data.\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray): A (batch size) x self.layers[0].num_nodes array\n",
    "          labels (np.ndarray): An array of ground-truth output labels, \n",
    "            length is the batch size.\n",
    "        \"\"\"\n",
    "        predicted = self.feedforward(inputs)\n",
    "        self.backProp(predicted, labels)\n",
    "        return predicted\n",
    "    \n",
    "    def train_epochs(self, inputs, labels, epochs = 50, num_batches = 10):\n",
    "        kf = KFold(n_splits=num_batches)\n",
    "        random.shuffle(inputs)\n",
    "        mini_batches = [inputs]\n",
    "        for i in range(epochs):\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                predicted = self.feedforward(X[train_index])\n",
    "                self.backProp(predicted, y[train_index])\n",
    "        predicted = self.predict(X)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FullyConnectedNeuralNetwork(\n",
    "    layers=[Layer(1),Layer(5, ReLU()), Layer(4, ReLU()), Layer(1)],\n",
    "    loss = SquaredErrorLoss(),\n",
    "    learning_rate=0.00001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.matrix(diabetes.data[:, np.newaxis, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.matrix(diabetes.target).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = network.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = network.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = network.train_epochs(X,y, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-123.4295279 ],\n",
       "        [ -47.4295279 ],\n",
       "        [-113.4295279 ],\n",
       "        [-178.45704117],\n",
       "        [-107.41792772],\n",
       "        [ -69.4295279 ],\n",
       "        [-110.36819166],\n",
       "        [ -35.41792772],\n",
       "        [ -82.4295279 ],\n",
       "        [-282.39006492],\n",
       "        [ -73.36819166],\n",
       "        [ -41.39006492],\n",
       "        [-151.45704117],\n",
       "        [-157.24424317],\n",
       "        [ -90.39006492],\n",
       "        [-143.41792772],\n",
       "        [-138.35360948],\n",
       "        [-116.35360948],\n",
       "        [ -69.45812253],\n",
       "        [-140.36819166],\n",
       "        [ -40.41792772],\n",
       "        [ -21.41792772],\n",
       "        [ -40.46048032],\n",
       "        [-217.41792772],\n",
       "        [-156.41792772],\n",
       "        [-174.41792772],\n",
       "        [-109.35360948],\n",
       "        [ -57.41792772],\n",
       "        [-103.41792772],\n",
       "        [-255.41792772],\n",
       "        [-101.36819166],\n",
       "        [ -31.41792772],\n",
       "        [-313.41792772],\n",
       "        [ -59.35725502],\n",
       "        [ -37.41792772],\n",
       "        [ -74.33538176],\n",
       "        [-237.41792772],\n",
       "        [-248.41792772],\n",
       "        [-224.42660115],\n",
       "        [ -62.40464709],\n",
       "        [ -72.40464709],\n",
       "        [ -27.43124748],\n",
       "        [ -33.41792772],\n",
       "        [ -64.46048032],\n",
       "        [-231.41792772],\n",
       "        [ -25.33538176],\n",
       "        [-162.45704117],\n",
       "        [-114.41792772],\n",
       "        [ -47.41792772],\n",
       "        [-114.41792772],\n",
       "        [-127.26611643],\n",
       "        [-197.26611643],\n",
       "        [ -31.43023823],\n",
       "        [ -76.41792772],\n",
       "        [-154.29892633],\n",
       "        [-100.41792772],\n",
       "        [ -24.41792772],\n",
       "        [  -9.43296706],\n",
       "        [-142.41792772],\n",
       "        [-142.32807274],\n",
       "        [ -33.3135085 ],\n",
       "        [-116.40829263],\n",
       "        [ -24.3135085 ],\n",
       "        [-100.41792772],\n",
       "        [ -43.41792772],\n",
       "        [-135.41792772],\n",
       "        [-122.36819166],\n",
       "        [ -69.45704117],\n",
       "        [-132.42781351],\n",
       "        [-150.41932698],\n",
       "        [ -20.33538176],\n",
       "        [-242.41792772],\n",
       "        [-174.41792772],\n",
       "        [ -83.3135085 ],\n",
       "        [ -57.43296706],\n",
       "        [ -14.3135085 ],\n",
       "        [-142.41792772],\n",
       "        [-172.45448545],\n",
       "        [-224.41932698],\n",
       "        [ -85.36090057],\n",
       "        [-115.41792772],\n",
       "        [ -23.41792772],\n",
       "        [ -24.41792772],\n",
       "        [-182.33538176],\n",
       "        [ -37.41792772],\n",
       "        [-113.41792772],\n",
       "        [ -27.40829263],\n",
       "        [-106.45206073],\n",
       "        [ -14.41792772],\n",
       "        [ -83.36454611],\n",
       "        [ -70.36819166],\n",
       "        [-136.41792772],\n",
       "        [ -20.41792772],\n",
       "        [ -68.29163524],\n",
       "        [ -62.41792772],\n",
       "        [-134.41792772],\n",
       "        [-122.41932698],\n",
       "        [-251.40829263],\n",
       "        [ -64.43296706],\n",
       "        [ -55.42538879],\n",
       "        [-100.40464709],\n",
       "        [ -74.41792772],\n",
       "        [-274.41792772],\n",
       "        [-170.41792772],\n",
       "        [ -67.26611643],\n",
       "        [ -25.40100155],\n",
       "        [-106.43124748],\n",
       "        [-116.39006492],\n",
       "        [-204.41792772],\n",
       "        [ -53.42660115],\n",
       "        [ -76.41792772],\n",
       "        [ -31.41792772],\n",
       "        [-218.33173622],\n",
       "        [-269.42436916],\n",
       "        [-230.41792772],\n",
       "        [-201.41792772],\n",
       "        [-247.43468664],\n",
       "        [-253.41792772],\n",
       "        [-151.41792772],\n",
       "        [-172.43387532],\n",
       "        [-172.41792772],\n",
       "        [-145.36819166],\n",
       "        [-152.41792772],\n",
       "        [ -56.41792772],\n",
       "        [ -93.41792772],\n",
       "        [-133.27340752],\n",
       "        [ -71.41792772],\n",
       "        [ -81.41792772],\n",
       "        [ -87.44114948],\n",
       "        [-240.41932698],\n",
       "        [-246.42538879],\n",
       "        [-130.42436916],\n",
       "        [ -79.41792772],\n",
       "        [ -55.43023823],\n",
       "        [ -75.37548274],\n",
       "        [-244.33173622],\n",
       "        [ -57.41792772],\n",
       "        [-252.35725502],\n",
       "        [-308.46054726],\n",
       "        [-253.41792772],\n",
       "        [ -90.43023823],\n",
       "        [-289.41792772],\n",
       "        [-207.41792772],\n",
       "        [ -32.41792772],\n",
       "        [-146.41792772],\n",
       "        [-231.41792772],\n",
       "        [-150.33173622],\n",
       "        [-100.42660115],\n",
       "        [ -68.45206073],\n",
       "        [ -98.39857548],\n",
       "        [-260.29163524],\n",
       "        [ -60.43266295],\n",
       "        [-264.31715404],\n",
       "        [ -43.35214684],\n",
       "        [-169.43387532],\n",
       "        [-158.43387532],\n",
       "        [   2.70836476],\n",
       "        [ -56.43124748],\n",
       "        [ -68.45532159],\n",
       "        [-167.41792772],\n",
       "        [ -25.40100155],\n",
       "        [-189.45532159],\n",
       "        [-144.41932698],\n",
       "        [-103.45812253],\n",
       "        [-186.43023823],\n",
       "        [ -31.42053934],\n",
       "        [ -42.41405169],\n",
       "        [-192.40829263],\n",
       "        [-240.41792772],\n",
       "        [-124.41792772],\n",
       "        [ -19.43145059],\n",
       "        [ -46.45704117],\n",
       "        [-267.42436916],\n",
       "        [ -73.41792772],\n",
       "        [-123.41792772],\n",
       "        [ -99.46054726],\n",
       "        [-209.41792772],\n",
       "        [-197.29163524],\n",
       "        [ -53.41405169],\n",
       "        [-123.32079959],\n",
       "        [ -79.41792772],\n",
       "        [ -36.43387532],\n",
       "        [-110.41932698],\n",
       "        [-157.41405169],\n",
       "        [-237.35042726],\n",
       "        [ -73.41792772],\n",
       "        [-109.45327309],\n",
       "        [-115.4375124 ],\n",
       "        [-113.41792772],\n",
       "        [ -51.41792772],\n",
       "        [-264.27340752],\n",
       "        [-150.41792772],\n",
       "        [ -63.42660115],\n",
       "        [ -88.41792772],\n",
       "        [ -58.42660115],\n",
       "        [ -94.39371046],\n",
       "        [ -44.41792772],\n",
       "        [-101.45327309],\n",
       "        [-114.45876075],\n",
       "        [ -62.35360948],\n",
       "        [-130.45448545],\n",
       "        [ -11.41792772],\n",
       "        [-168.45876075],\n",
       "        [-194.3718372 ],\n",
       "        [-249.41792772],\n",
       "        [ -71.38641937],\n",
       "        [-168.41932698],\n",
       "        [-174.36454611],\n",
       "        [-127.4375124 ],\n",
       "        [ -49.41792772],\n",
       "        [-163.41792772],\n",
       "        [ -42.41932698],\n",
       "        [ -45.41792772],\n",
       "        [ -21.41792772],\n",
       "        [ -37.42053934],\n",
       "        [-235.44599892],\n",
       "        [-220.46048032],\n",
       "        [-268.45704117],\n",
       "        [-186.41792772],\n",
       "        [-157.43124748],\n",
       "        [ -50.32807274],\n",
       "        [ -65.45532159],\n",
       "        [-224.25882535],\n",
       "        [-122.45704117],\n",
       "        [ -49.4593349 ],\n",
       "        [-180.42436916],\n",
       "        [ -49.39857548],\n",
       "        [ -80.32444513],\n",
       "        [-132.41792772],\n",
       "        [ -25.4435742 ],\n",
       "        [-192.41792772],\n",
       "        [-126.41792772],\n",
       "        [-231.41792772],\n",
       "        [ -62.31715404],\n",
       "        [-218.37794053],\n",
       "        [ -96.3718372 ],\n",
       "        [ -39.43296706],\n",
       "        [ -44.41792772],\n",
       "        [-229.28069861],\n",
       "        [-234.43387532],\n",
       "        [-247.41932698],\n",
       "        [-149.41792772],\n",
       "        [ -43.41558372],\n",
       "        [ -19.41792772],\n",
       "        [-159.45532159],\n",
       "        [ -97.43124748],\n",
       "        [ -50.41405169],\n",
       "        [ -23.4435742 ],\n",
       "        [-230.43387532],\n",
       "        [-187.41792772],\n",
       "        [-275.41792772],\n",
       "        [-215.33902731],\n",
       "        [ -63.45812253],\n",
       "        [-122.45569781],\n",
       "        [-282.41792772],\n",
       "        [-125.41792772],\n",
       "        [-318.41792772],\n",
       "        [ -35.41792772],\n",
       "        [ -61.43023823],\n",
       "        [ -22.42781351],\n",
       "        [ -11.43993712],\n",
       "        [ -75.30621741],\n",
       "        [-280.40464709],\n",
       "        [ -88.37794053],\n",
       "        [-117.42538879],\n",
       "        [ -46.40889295],\n",
       "        [ -17.41792772],\n",
       "        [ -87.43296706],\n",
       "        [-236.41932698],\n",
       "        [ -59.28069861],\n",
       "        [-174.32807274],\n",
       "        [ -99.36819166],\n",
       "        [-154.45188243],\n",
       "        [-213.41792772],\n",
       "        [ -38.42608874],\n",
       "        [ -66.4295279 ],\n",
       "        [-255.41792772],\n",
       "        [ -36.4435742 ],\n",
       "        [ -74.45704117],\n",
       "        [-172.397356  ],\n",
       "        [-237.26611643],\n",
       "        [ -66.44844327],\n",
       "        [-202.45704117],\n",
       "        [-153.25153426],\n",
       "        [-128.41792772],\n",
       "        [-205.44156495],\n",
       "        [ -32.41405169],\n",
       "        [-191.41792772],\n",
       "        [ -52.44236184],\n",
       "        [ -40.43023823],\n",
       "        [-304.42436916],\n",
       "        [-220.44963601],\n",
       "        [ -56.41233211],\n",
       "        [-172.45360201],\n",
       "        [ -27.42417643],\n",
       "        [ -57.41792772],\n",
       "        [ -61.41792772],\n",
       "        [  -3.4435742 ],\n",
       "        [-101.41792772],\n",
       "        [ -55.45327309],\n",
       "        [-247.32444513],\n",
       "        [ -37.41792772],\n",
       "        [-170.41792772],\n",
       "        [-208.45812253],\n",
       "        [-225.41792772],\n",
       "        [ -96.43296706],\n",
       "        [ -16.41792772],\n",
       "        [-144.45704117],\n",
       "        [ -86.42175171],\n",
       "        [-114.3718372 ],\n",
       "        [ -81.30621741],\n",
       "        [-152.41792772],\n",
       "        [-116.4381258 ],\n",
       "        [-135.39006492],\n",
       "        [-119.43124748],\n",
       "        [ -69.35214684],\n",
       "        [-192.41828099],\n",
       "        [-162.42538879],\n",
       "        [ -81.41792772],\n",
       "        [-163.40464709],\n",
       "        [ -94.4435742 ],\n",
       "        [-202.43124748],\n",
       "        [-214.4593349 ],\n",
       "        [-220.32809068],\n",
       "        [-221.41792772],\n",
       "        [-164.30986296],\n",
       "        [-103.41792772],\n",
       "        [-209.42417643],\n",
       "        [ -50.41792772],\n",
       "        [-107.34996394],\n",
       "        [-216.43296706],\n",
       "        [-171.41792772],\n",
       "        [-242.33902731],\n",
       "        [-136.41792772],\n",
       "        [ -44.41792772],\n",
       "        [ -68.41792772],\n",
       "        [-278.4295279 ],\n",
       "        [ -63.44500411],\n",
       "        [-186.38277383],\n",
       "        [ -67.43124748],\n",
       "        [-188.44844327],\n",
       "        [-235.43993712],\n",
       "        [-150.43266295],\n",
       "        [ -85.42608874],\n",
       "        [-172.4593349 ],\n",
       "        [-111.43296706],\n",
       "        [-111.36454611],\n",
       "        [ -60.44842365],\n",
       "        [-120.46048032],\n",
       "        [ -60.41792772],\n",
       "        [-215.41792772],\n",
       "        [ -43.41792772],\n",
       "        [ -49.46054726],\n",
       "        [ -81.41792772],\n",
       "        [-244.37106221],\n",
       "        [ -32.41792772],\n",
       "        [ -26.41792772],\n",
       "        [-193.36090057],\n",
       "        [ -62.44236184],\n",
       "        [-283.41792772],\n",
       "        [-253.34996394],\n",
       "        [-154.41792772],\n",
       "        [-293.37966011],\n",
       "        [ -30.40100155],\n",
       "        [-234.41792772],\n",
       "        [-178.43630004],\n",
       "        [-205.41792772],\n",
       "        [-214.41792772],\n",
       "        [ -95.41792772],\n",
       "        [-139.41792772],\n",
       "        [ -35.25882535],\n",
       "        [-169.45876075],\n",
       "        [ -43.33538176],\n",
       "        [-140.44114948],\n",
       "        [-112.41792772],\n",
       "        [-189.26611643],\n",
       "        [ -93.45448545],\n",
       "        [-207.43296706],\n",
       "        [-217.41792772],\n",
       "        [ -12.41792772],\n",
       "        [ -24.41792772],\n",
       "        [ -76.41233211],\n",
       "        [-104.44599892],\n",
       "        [ -60.43630004],\n",
       "        [ -41.37450137],\n",
       "        [-191.41792772],\n",
       "        [ -44.4435742 ],\n",
       "        [-173.43630004],\n",
       "        [ -82.44236184],\n",
       "        [ -23.32809068],\n",
       "        [-249.45691017],\n",
       "        [ -35.41792772],\n",
       "        [ -90.31715404],\n",
       "        [ -41.44963601],\n",
       "        [-245.43124748],\n",
       "        [-230.397356  ],\n",
       "        [ -15.45691017],\n",
       "        [-170.41932698],\n",
       "        [-214.42417643],\n",
       "        [-204.41792772],\n",
       "        [-147.38641937],\n",
       "        [ -65.4381258 ],\n",
       "        [-140.39006492],\n",
       "        [-247.43023823],\n",
       "        [-265.3135085 ],\n",
       "        [-253.41792772],\n",
       "        [ -44.41792772],\n",
       "        [-112.36074474],\n",
       "        [-161.43266295],\n",
       "        [-153.42660115],\n",
       "        [-181.37106221],\n",
       "        [-108.41792772],\n",
       "        [-233.41061253],\n",
       "        [ -85.38653842],\n",
       "        [-103.41932698],\n",
       "        [-146.41792772],\n",
       "        [-229.45188243],\n",
       "        [ -27.36074474],\n",
       "        [ -56.41792772],\n",
       "        [ -14.41558372],\n",
       "        [-118.41828099],\n",
       "        [-184.43984538],\n",
       "        [-205.41792772],\n",
       "        [ -63.4435742 ],\n",
       "        [ -83.41792772],\n",
       "        [-124.40829263],\n",
       "        [ -92.36819166],\n",
       "        [ -39.45532159],\n",
       "        [-282.25153426],\n",
       "        [ -66.41792772],\n",
       "        [-155.31715404],\n",
       "        [ -38.22601545],\n",
       "        [-145.41792772],\n",
       "        [ -44.41792772],\n",
       "        [ -21.41792772],\n",
       "        [ -36.41405169],\n",
       "        [ -20.35360948],\n",
       "        [-150.41792772],\n",
       "        [ -76.41792772],\n",
       "        [-104.25585041],\n",
       "        [-192.29163524],\n",
       "        [ -29.41792772]])"
      ]
     },
     "execution_count": 1061,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
