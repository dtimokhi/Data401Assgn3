{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "## Printed copy due in class on November 14, 2018\n",
    "\n",
    "You may work in pairs on this assignment. You are not permitted to discuss this assignment with anyone other than your partner or the instructors.\n",
    "\n",
    "### Student 1: Dmitriy Timokhin\n",
    "### Student 2: Hanson Egbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Neural Networks by Hand\n",
    "\n",
    "## Part A\n",
    "\n",
    "Suppose we train a neural network using the ReLU activation function: \n",
    "\n",
    "$$ g(a) = \\max(a, 0). $$\n",
    "\n",
    "1. Draw the graph of $g(a)$. (Matplotlib plots are acceptable.)\n",
    "2. What is the derivative $h = g'(a)$ in terms of the input $a$?\n",
    "3. What is the derivative $h = g'(a)$ in terms of the input $h$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ReLU(a): \n",
    "    return np.max([a, 0])\n",
    "\n",
    "def ReLUderiv(a):\n",
    "    if a <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "ReLUderivVect = np.vectorize(ReLUderiv)\n",
    "\n",
    "reluVect = np.vectorize(ReLU)\n",
    "vals = np.arange(-20, 20)\n",
    "plt.plot(vals, reluVect(vals));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)\n",
    "\n",
    "Derivative when a < 0: 0 <br>\n",
    "Derivative when a  = 0: Undefined <br>\n",
    "Derivative when a > 0: 1 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)\n",
    "If h is 0: Then a must be < 0 <br>\n",
    "If h is 1: Then a must be > 0 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Consider a neural network, where the current weights and biases are as follows. (Since this network is so simple, we omit the superscripts $w^{(m)}$ and $b^{(m)}$ that indicates which layer each weight/bias is in.)\n",
    "\n",
    "![](../Data401Fall2018/neural_network.png)\n",
    "\n",
    "The activation function used in the hidden layer is the ReLU activation function. \n",
    "**No activation function is used in the output layer.**\n",
    "\n",
    "Predict the output for the input ${\\bf x} = (-1, 1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Since we are including Bias in our network the values of each layer will follow\n",
    "## a format similar to g(wx + b) where g is the ReLU function as specified.\n",
    "\n",
    "## We can create a vector for our first layer\n",
    "x = np.matrix([-1 ,1]).T  # shape is 2, 1\n",
    "\n",
    "## Weight matrix for layer 1\n",
    "w1 = np.matrix([[-1,2], [1,0]])\n",
    "\n",
    "## First layer (using ReLU function from previous problem)\n",
    "a1 = np.matmul(w1, x)\n",
    "h = reluVect(a1)\n",
    "\n",
    "## Final Layer\n",
    "w2 = np.matrix([1,-1])\n",
    "out = np.matmul(w2, h)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted output for the input x = (-1, 1) will be 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "We are training the neural network from the previous part to minimize the loss function \n",
    "\n",
    "$$ L_i({\\bf w}, {\\bf b}) = (\\hat y_i - y_i)^2. $$\n",
    "\n",
    "You have a new training observation ${\\bf x_i} = (-1, 1)$ and $y_i = 1$. \n",
    "Use one step of (stochastic) gradient descent to update the 6 weights (from the previous part) ${\\bf w} = (w_{11}, w_{12}, w_{21}, w_{22}, w_1, w_2)$ and the 3 biases (from the previous part) ${\\bf b} = (b_1, b_2, b)$, based on this single observation. Use a learning rate of $\\eta = 0.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since we are performing stochastic gradient descent on the neural network,\n",
    "## we are in essense performing backpropogation.\n",
    "## We can do this at each layer and reuse our stored values for future use.\n",
    "\n",
    "lrate = 0.25\n",
    "## Firstly we have our predicted yhat from our previous, part b, which we decided is 3\n",
    "y = 1\n",
    "yhat = 3\n",
    "## This is our dl/dy value which will be useful later\n",
    "dldy = 2 * (3 - 1)\n",
    "## This is just the derivative of wh + b with respect to w which is h\n",
    "dydw = h\n",
    "\n",
    "#### Updating w1, and w2 (stored in w2 for my code)\n",
    "w2_n = w2.T - lrate * dldy * dydw\n",
    "\n",
    "## Updating the Bias term b\n",
    "b_n = np.matrix([0]) - lrate * dldy * 1\n",
    "\n",
    "### Updating w11, w12, w21, w22. We need dldy, dydh, and dhdw\n",
    "## We can reuse dldy from the previous weight updates\n",
    "\n",
    "## This is just the derivative of wh + b with respect to h so its just w\n",
    "dydh = w2.T\n",
    "## We know h is just h=g(wx + b) so dhdw\n",
    "## is just g'(wx + b) * x\n",
    "# dhdw - dhda dadw\n",
    "dhda = ReLUderivVect(a1)\n",
    "dadw = x.T\n",
    "dhdw = dhda * dadw\n",
    "w1_n = w1 - lrate * dldy * np.matrix(np.array(dydh) * np.array(dhda)) * dadw\n",
    "\n",
    "#### Updating b1, b2\n",
    "b_1_n = np.matrix([0,0]).T - lrate * dldy * dydh * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing new Neural Network\n",
    "h_n = reluVect(np.matmul(w1_n, x) + b_1_n)\n",
    "out_n = np.matmul(w2_n.T, h_n) + b_n\n",
    "out_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Implementing Neural Networks\n",
    "\n",
    "This question will guide you through the implementation of a fully-connected neural network. Although the question is divided into parts, you need not do the parts in order. In fact, you may want to start with Part C (implementing the neural network itself) so that you see how the pieces fit together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Loss Functions\n",
    "\n",
    "By subclassing the `Loss` class below, implement the squared-error loss function \n",
    "\n",
    "$$L({\\bf \\hat y}, {\\bf y}) = \\sum_i (\\hat y_i - y_i)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        \"\"\"Calculates the loss as a function of the prediction and the actual.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (float) \n",
    "          The value of the loss for this batch of observations.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def derivative(self, predicted, actual):\n",
    "        \"\"\"The derivative of the loss with respect to the prediction.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives of the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "class SquaredErrorLoss(Loss):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        return np.sum(\n",
    "            (predicted - actual) ** 2\n",
    "        )\n",
    "    \n",
    "    def derivative(self, predicted, actual):\n",
    "        return (\n",
    "            2 * (predicted - actual)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Activation Functions\n",
    "\n",
    "By subclassing the `ActivationFunction` class below, implement the ReLU and Sigmoid activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(object):\n",
    "        \n",
    "    def __call__(self, a):\n",
    "        \"\"\"Applies activation function to the values in a layer.\n",
    "        \n",
    "        Args:\n",
    "          a (np.ndarray, float): the values from the previous layer (after \n",
    "            multiplying by the weights.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The values h = g(a).\n",
    "        \"\"\"\n",
    "        return a\n",
    "    \n",
    "    def derivative(self, h):\n",
    "        \"\"\"The derivatives as a function of the outputs at the nodes.\n",
    "        \n",
    "        Args:\n",
    "          h (np.ndarray, float): the outputs h = g(a) at the nodes.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives dh/da.\n",
    "        \"\"\"\n",
    "        return np.ones(h.shape)\n",
    "    \n",
    "class ReLU(ActivationFunction):\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        return np.where(a > 0, a, 0)\n",
    "    \n",
    "    def derivative(self, a):\n",
    "        return np.where(a > 0, 1, 0)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        return 1/(1 + np.exp(-a))\n",
    "    \n",
    "    def derivative(self, a):\n",
    "        return  self.__call__(a) * (1 - self.__call__(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ActivationFunction()\n",
    "x.derivative(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Putting It All Together\n",
    "\n",
    "A `Layer` class has been completely implemented for you. This class applies the activation function to the incoming values and stores the current values at the layer. (We need to store the values because they come in handy when we are doing backpropagation.)\n",
    "\n",
    "The `FullyConnectedNeuralNetwork` class is only partially implemented. You have to implement the `feedforward` and `backprop` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"A data structure for a layer in a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      num_nodes (int): number of nodes in the layer\n",
    "      activation_function (ActivationFunction)\n",
    "      values_pre_activation (np.ndarray, float): most recent values\n",
    "        in layer, before applying activation function\n",
    "      values_post_activation (np.ndarray, float): most recent values\n",
    "        in layer, after applying activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, activation_function=ActivationFunction()):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def get_layer_values(self, values_pre_activation):\n",
    "        \"\"\"Applies activation function to values from previous layer.\n",
    "        \n",
    "        Stores the values (both before and after applying activation \n",
    "        function)\n",
    "        \n",
    "        Args:\n",
    "          values_pre_activation (np.ndarray, float): \n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer before applying the activation function\n",
    "        \n",
    "        Returns: (np.ndarray, float)\n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer after applying the activation function\n",
    "        \"\"\"\n",
    "        self.values_pre_activation = values_pre_activation\n",
    "        self.values_post_activation = self.activation_function(\n",
    "            values_pre_activation\n",
    "        )\n",
    "        return self.values_post_activation\n",
    "\n",
    "        \n",
    "class FullyConnectedNeuralNetwork(object):\n",
    "    \"\"\"A data structure for a fully-connected neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      layers (Layer): A list of Layer objects.\n",
    "      loss (Loss): The loss function to use in training.\n",
    "      learning_rate (float): The learning rate to use in backpropagation.\n",
    "      weights (list, np.ndarray): A list of weight matrices,\n",
    "        length should be len(self.layers) - 1\n",
    "      biases (list, float): A list of bias terms,\n",
    "        length should be equal to len(self.layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, loss, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weight matrices and biases to zeros\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.weights.append(\n",
    "                np.zeros((self.layers[i - 1].num_nodes, self.layers[i].num_nodes))\n",
    "            )\n",
    "            self.biases.append(np.zeros(self.layers[i].num_nodes))\n",
    "        print(self.biases)\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"Predicts the output(s) for a given set of input(s).\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray, float): A (batch size) x self.layers[0].num_nodes array\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          An array of the predicted output labels, length is the batch size\n",
    "        \"\"\"\n",
    "        self.storedValues = []\n",
    "        h = inputs\n",
    "        ## Iterate layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ## g(hw + b),  h = previous layer values\n",
    "            if i != len(self.layers) - 1:\n",
    "                h = layer.get_layer_values(np.matrix(np.add(h * self.weights[i], np.matrix(self.biases[i]))))\n",
    "        return h     \n",
    "        \n",
    "    def backprop(self, predicted, actual):\n",
    "        \"\"\"Updates self.weights and self.biases based on predicted and actual values.\n",
    "        \n",
    "        This will require using the values at each layer that were stored at the\n",
    "        feedforward step.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): An array of the predicted output labels\n",
    "          actual (np.ndarray, float): An array of the actual output labels\n",
    "        \"\"\"\n",
    "        # TODO: Implement backpropagation.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train(self, inputs, labels):\n",
    "        \"\"\"Trains neural network based on a batch of training data.\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray): A (batch size) x self.layers[0].num_nodes array\n",
    "          labels (np.ndarray): An array of ground-truth output labels, \n",
    "            length is the batch size.\n",
    "        \"\"\"\n",
    "        predicted = self.feedforward(inputs)\n",
    "        self.backprop(predicted, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3, -4])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Layer(2)\n",
    "x.get_layer_values(np.array([1,2,3,-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.,  0.]), array([ 0.])]\n"
     ]
    }
   ],
   "source": [
    "network = FullyConnectedNeuralNetwork(\n",
    "    layers=[Layer(2), Layer(2, ReLU()), Layer(1)],\n",
    "    loss = SquaredErrorLoss,\n",
    "    learning_rate=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 0.]\n",
      " [ 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.feedforward(np.matrix([[-1, 1],[-2,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 4,  8],\n",
       "        [13, 21]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(np.matrix([[1,2], [10,15]]), np.matrix([3,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Testing It Out\n",
    "\n",
    "Try testing out your neural network implmentation on the simple neural network you considered in Question 1. The following code initializes a neural network with the structure of the network from Question 1. Note that all weights are initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FullyConnectedNeuralNetwork(\n",
    "    layers=[Layer(2), Layer(2, ReLU()), Layer(1)],\n",
    "    learning_rate=0.25\n",
    ")\n",
    "\n",
    "network.train(np.array([[-1, 1]]), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
